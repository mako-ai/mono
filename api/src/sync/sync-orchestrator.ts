import { syncConnectorRegistry } from "./connector-registry";
import { databaseDataSourceManager } from "./database-data-source-manager";
import { getDestinationManager } from "./destination-manager";
import {
  databaseConnectionService,
  ConnectionConfig,
} from "../services/database-connection.service";
import { SyncLogger, FetchState } from "../connectors/base/BaseConnector";
import { Db } from "mongodb";
import { ProgressReporter } from "./progress-reporter";
import axios from "axios";

export interface SyncChunkResult {
  state: FetchState;
  entity: string;
  collectionName: string;
  completed: boolean;
}

export interface SyncChunkOptions {
  dataSourceId: string;
  destinationId: string;
  entity: string;
  isIncremental: boolean;
  state?: FetchState;
  maxIterations?: number;
  logger?: SyncLogger;
  step?: any; // Inngest step object for serverless-friendly retries
}

/**
 * Execute an operation with retry logic
 */
async function executeWithRetry<T>(
  operation: () => Promise<T>,
  maxRetries: number = 3,
  logger?: SyncLogger,
  step?: any, // Inngest step object
  operationName?: string, // For unique step names in Inngest
  rateLimitDelayMs: number = 200, // Base delay from data source rate limit settings
): Promise<T> {
  let attempts = 0;

  while (attempts <= maxRetries) {
    try {
      return await operation();
    } catch (error) {
      attempts++;

      if (attempts > maxRetries) {
        throw error;
      }

      // Calculate exponential backoff delay (2^attempts * rate_limit_delay_ms base)
      let delayMs: number;

      // Handle rate limiting with specific delay
      if (axios.isAxiosError(error) && error.response?.status === 429) {
        const retryAfter = error.response.headers["retry-after"];
        delayMs = retryAfter
          ? parseInt(retryAfter, 10) * 1000
          : Math.min(rateLimitDelayMs * Math.pow(2, attempts), 30000); // Cap at 30 seconds
        logger?.log(
          "warn",
          `Rate limited. Waiting ${delayMs}ms before retry ${attempts}/${maxRetries}`,
        );
      } else if (isRetryableError(error)) {
        // Exponential backoff for other retryable errors (2x each time, starting from rate limit delay)
        delayMs = Math.min(rateLimitDelayMs * Math.pow(2, attempts), 30000); // Cap at 30 seconds
        logger?.log(
          "warn",
          `Retryable error (${getErrorDescription(error)}). Waiting ${delayMs}ms before retry ${attempts}/${maxRetries}`,
        );
      } else {
        // Non-retryable error
        throw error;
      }

      // Use Inngest step.sleep if available (serverless-friendly), otherwise fall back to setTimeout
      if (step && step.sleep) {
        const sleepStepName = `retry-delay-${operationName || "operation"}-${attempts}`;
        await step.sleep(sleepStepName, delayMs);
      } else {
        await sleep(delayMs);
      }
    }
  }

  throw new Error("Max retries exceeded");
}

/**
 * Check if error is retryable
 */
function isRetryableError(error: any): boolean {
  if (axios.isAxiosError(error)) {
    if (!error.response) {
      // Network errors are retryable
      return true;
    }
    // Retry on server errors, rate limiting, and gateway timeouts
    const status = error.response.status;
    return status >= 500 || status === 429 || status === 408; // 408 = Request Timeout
  }
  return false;
}

/**
 * Get human-readable error description
 */
function getErrorDescription(error: any): string {
  if (axios.isAxiosError(error)) {
    if (!error.response) {
      return "Network error";
    }
    const status = error.response.status;
    switch (status) {
      case 500:
        return "Internal Server Error";
      case 502:
        return "Bad Gateway";
      case 503:
        return "Service Unavailable";
      case 504:
        return "Gateway Timeout";
      case 408:
        return "Request Timeout";
      default:
        return `HTTP ${status}`;
    }
  }
  return "Unknown error";
}

/**
 * Sleep utility function
 */
function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

/**
 * Ensure required indexes exist on a collection for efficient sync operations
 */
async function ensureCollectionIndexes(
  collection: any,
  logger?: SyncLogger,
): Promise<void> {
  try {
    // Get existing indexes
    const existingIndexes = await collection.indexes();
    const existingIndexNames = existingIndexes.map((idx: any) => idx.name);

    // 1. Unique index on 'id' field for webhook updates
    if (
      !existingIndexNames.includes("id_unique_idx") &&
      !existingIndexNames.includes("id_1")
    ) {
      await collection.createIndex(
        { id: 1 },
        {
          unique: true,
          background: true,
          name: "id_unique_idx",
          partialFilterExpression: { id: { $exists: true } },
        },
      );
      logger?.log(
        "info",
        `Created unique id index for ${collection.collectionName}`,
      );
    }

    // 2. Compound index for bulk sync upserts (uses both id and _dataSourceId)
    if (!existingIndexNames.includes("sync_upsert_idx")) {
      await collection.createIndex(
        { id: 1, _dataSourceId: 1 },
        {
          background: true,
          name: "sync_upsert_idx",
        },
      );
      logger?.log(
        "info",
        `Created sync upsert index for ${collection.collectionName}`,
      );
    }

    // 3. Index for incremental sync date queries
    if (!existingIndexNames.includes("incremental_sync_idx")) {
      await collection.createIndex(
        { _dataSourceId: 1, _syncedAt: -1 },
        {
          background: true,
          name: "incremental_sync_idx",
        },
      );
      logger?.log(
        "info",
        `Created incremental sync index for ${collection.collectionName}`,
      );
    }
  } catch (error) {
    logger?.log(
      "warn",
      `Failed to create indexes: ${error instanceof Error ? error.message : String(error)}`,
    );
    // Don't throw - indexes are for performance, not correctness
  }
}

/**
 * Performs a single chunk of sync work and returns state for resumption
 */
export async function performSyncChunk(
  options: SyncChunkOptions,
): Promise<SyncChunkResult> {
  const {
    dataSourceId,
    destinationId,
    entity,
    isIncremental,
    state,
    maxIterations = 10,
    logger,
  } = options;

  const syncMode = isIncremental ? "incremental" : "full";
  let db: Db | null = null;

  try {
    // Get the data source
    const dataSource =
      await databaseDataSourceManager.getDataSource(dataSourceId);
    if (!dataSource) {
      throw new Error(`Data source '${dataSourceId}' not found`);
    }

    if (!dataSource.active) {
      throw new Error(`Data source '${dataSource.name}' is not active`);
    }

    // Get connector from registry
    const connector = await syncConnectorRegistry.getConnector(dataSource);
    if (!connector) {
      throw new Error(
        `Failed to create connector for type: ${dataSource.type}`,
      );
    }

    // Check if connector supports resumable fetching
    if (!connector.supportsResumableFetching()) {
      throw new Error(
        `Connector ${dataSource.type} does not support resumable fetching`,
      );
    }

    // Get connection from unified pool
    const connection = await databaseConnectionService.getConnectionById(
      "destination",
      destinationId,
      async (id: string) => {
        const destinationDb = await getDestinationManager().getDestination(id);
        if (!destinationDb) return null;

        return {
          connectionString: destinationDb.connection.connection_string,
          database: destinationDb.connection.database,
        } as ConnectionConfig;
      },
    );
    db = connection.db;

    // Collection setup - if entity has sub-entity notation (e.g., activities:Call),
    // normalize to parent for collection naming so all activity types land together
    const normalizedEntityName = entity.includes(":")
      ? entity.split(":")[0]
      : entity;
    const collectionName = `${dataSource.name}_${normalizedEntityName}`;
    const stagingCollectionName = `${collectionName}_staging`;
    const useStaging = syncMode === "full"; // Use staging for ALL chunks of full sync

    const collection = useStaging
      ? db.collection(stagingCollectionName)
      : db.collection(collectionName);

    // Ensure indexes exist for incremental sync on first chunk
    if (!useStaging && !state) {
      await ensureCollectionIndexes(collection, logger);
    }

    if (useStaging && !state) {
      // Drop staging collection if exists (only on first chunk)
      try {
        await db.collection(stagingCollectionName).drop();
      } catch {
        // Ignore if doesn't exist
      }
      await db.createCollection(stagingCollectionName);

      // Create indexes on staging collection for efficient inserts
      const stagingCollection = db.collection(stagingCollectionName);
      await ensureCollectionIndexes(stagingCollection, logger);
    }

    let lastSyncDate: Date | undefined;

    // Get last sync date for incremental (only on first chunk)
    if (syncMode === "incremental" && !state) {
      const lastRecord = await db
        .collection(collectionName)
        .find({ _dataSourceId: dataSource.id })
        .sort({ _syncedAt: -1 })
        .limit(1)
        .toArray();

      if (lastRecord.length > 0) {
        lastSyncDate = lastRecord[0]._syncedAt;
        logger?.log(
          "info",
          `Syncing ${entity} updated after: ${lastSyncDate!.toISOString()}`,
        );
      }
    }

    // Create progress reporter
    const progressReporter = new ProgressReporter(entity, undefined, logger);

    // Fetch chunk from connector with retry logic
    const maxRetries = dataSource.settings?.max_retries || 3;
    const rateLimitDelay = dataSource.settings?.rate_limit_delay_ms || 200;
    const fetchState = await executeWithRetry(
      () =>
        connector.fetchEntityChunk({
          entity,
          state,
          maxIterations,
          ...(lastSyncDate && { since: lastSyncDate }),
          onBatch: async batch => {
            if (batch.length === 0) return;

            // Add metadata to records
            const processedRecords = batch.map(record => ({
              ...record,
              _dataSourceId: dataSource.id,
              _dataSourceName: dataSource.name,
              _syncedAt: new Date(),
            }));

            // Write to database with timing
            const bulkStart = Date.now(); // Declare outside for catch scope
            try {
              // Always use bulkWrite with upserts to handle duplicates gracefully
              const bulkOps = processedRecords.map(record => ({
                replaceOne: {
                  filter: {
                    id: record.id,
                    _dataSourceId: dataSource.id,
                  },
                  replacement: record,
                  upsert: true,
                },
              }));
              const result = await collection.bulkWrite(bulkOps, {
                ordered: false,
              });
              const syncType = useStaging ? "full sync" : "incremental sync";
              console.log(`MongoDB bulkWrite upsert mode used (${syncType})`);
              const bulkDuration = Date.now() - bulkStart;
              console.log(
                `MongoDB write succeeded: ${result.upsertedCount} upserted, ${result.modifiedCount} modified, took ${bulkDuration}ms for ${batch.length} records`,
              );
            } catch (bulkError: any) {
              const bulkDuration = Date.now() - bulkStart;
              console.error(
                `MongoDB write failed after ${bulkDuration}ms: ${bulkError.message}`,
              );
              throw bulkError;
            }
          },
          onProgress: (current, total) => {
            progressReporter.reportProgress(current, total);
          },
        }),
      maxRetries,
      logger,
      options.step, // Pass through the step from options for Inngest sleep
      `fetch-chunk-${entity}`,
      rateLimitDelay,
    );

    const completed = !fetchState.hasMore;

    if (completed) {
      progressReporter.reportComplete();

      // Hot swap for full sync (only when sync is completed)
      if (syncMode === "full") {
        try {
          await db.collection(collectionName).drop();
        } catch {
          // Ignore if doesn't exist
        }
        await db.collection(stagingCollectionName).rename(collectionName);
      }

      logger?.log(
        "info",
        `âœ… ${entity} sync completed (${fetchState.totalProcessed} records)`,
      );
    } else {
      logger?.log(
        "info",
        `ðŸ“Š ${entity} chunk completed (${fetchState.totalProcessed} records so far, ${fetchState.iterationsInChunk} iterations)`,
      );
    }

    return {
      state: fetchState,
      entity,
      collectionName,
      completed,
    };
  } catch (error) {
    const errorMsg = `Sync chunk failed: ${error instanceof Error ? error.message : String(error)}`;
    logger?.log("error", errorMsg, {
      stack: error instanceof Error ? error.stack : undefined,
    });
    throw new Error(errorMsg, { cause: error });
  }
  // Note: We don't close the connection here anymore - it stays in the unified pool
}

/**
 * Orchestrates the sync process using the new architecture
 * where connectors are database-agnostic and all DB operations
 * are handled by the sync layer
 */
export async function performSync(
  dataSourceId: string,
  destinationId: string,
  entities: string[] | undefined,
  isIncremental: boolean = false,
  logger?: SyncLogger,
  step?: any, // Inngest step object for serverless-friendly retries
) {
  logger?.log(
    "debug",
    `performSync called with isIncremental: ${isIncremental}`,
  );
  const syncMode = isIncremental ? "incremental" : "full";
  logger?.log("debug", `Sync mode determined as: ${syncMode}`);
  let db: Db | null = null;

  try {
    // Validate configuration
    const validation = databaseDataSourceManager.validateConfig();
    if (!validation.valid) {
      const errorMsg =
        "Configuration validation failed: " + validation.errors.join(", ");
      logger?.log("error", errorMsg);
      throw new Error(errorMsg);
    }

    // Get the data source
    const dataSource =
      await databaseDataSourceManager.getDataSource(dataSourceId);
    if (!dataSource) {
      const errorMsg = `Data source '${dataSourceId}' not found`;
      logger?.log("error", errorMsg);
      throw new Error(errorMsg);
    }

    if (!dataSource.active) {
      const errorMsg = `Data source '${dataSource.name}' is not active`;
      logger?.log("error", errorMsg);
      throw new Error(errorMsg);
    }

    // Get destination database (just for validation)
    const destinationDb =
      await getDestinationManager().getDestination(destinationId);
    if (!destinationDb) {
      const errorMsg = `Destination database '${destinationId}' not found`;
      logger?.log("error", errorMsg);
      throw new Error(errorMsg);
    }

    // Get connector from registry
    const connector = await syncConnectorRegistry.getConnector(dataSource);
    if (!connector) {
      const errorMsg = `Failed to create connector for type: ${dataSource.type}`;
      logger?.log("error", errorMsg);
      throw new Error(errorMsg);
    }

    // Test connection first with retry logic
    const maxRetries = dataSource.settings?.max_retries || 3;
    const rateLimitDelay = dataSource.settings?.rate_limit_delay_ms || 200;
    const connectionTest = await executeWithRetry(
      () => connector.testConnection(),
      maxRetries,
      logger,
      step, // Pass step parameter for Inngest sleep when available
      `test-connection-${dataSource.type}`,
      rateLimitDelay,
    );
    if (!connectionTest.success) {
      const errorMsg = `Failed to connect to ${dataSource.type}: ${connectionTest.message}`;
      logger?.log("error", errorMsg, { details: connectionTest.details });
      throw new Error(errorMsg);
    }

    logger?.log("info", `Successfully connected to ${dataSource.type}`);
    logger?.log("info", `Starting ${syncMode} sync...`);
    logger?.log("info", `Source: ${dataSource.name} (${dataSource.type})`);
    logger?.log("info", `Destination: ${destinationDb.name}`);

    const startTime = Date.now();

    // Get connection from unified pool
    const connection = await databaseConnectionService.getConnectionById(
      "destination",
      destinationId,
      async (id: string) => {
        const destinationDb = await getDestinationManager().getDestination(id);
        if (!destinationDb) return null;

        return {
          connectionString: destinationDb.connection.connection_string,
          database: destinationDb.connection.database,
        } as ConnectionConfig;
      },
    );
    db = connection.db;

    // Determine which entities to sync
    const availableEntities = connector.getAvailableEntities();
    let entitiesToSync: string[];

    if (entities && entities.length > 0) {
      // Validate requested entities
      const invalidEntities = entities.filter(
        e => !availableEntities.includes(e),
      );
      if (invalidEntities.length > 0) {
        const errorMsg = `Invalid entities for ${dataSource.type} connector: ${invalidEntities.join(", ")}. Available: ${availableEntities.join(", ")}`;
        logger?.log("error", errorMsg);
        throw new Error(errorMsg);
      }
      entitiesToSync = entities;
      logger?.log("info", `Entities: ${entitiesToSync.join(", ")}`);
    } else {
      // Sync all entities
      entitiesToSync = availableEntities;
      logger?.log("info", `Entities: All (${entitiesToSync.join(", ")})`);
    }

    // Sync each entity
    for (const entityName of entitiesToSync) {
      logger?.log("info", `Syncing entity: ${entityName}`);

      // Perform sync using clean architecture
      // Normalize sub-entity notation (e.g., activities:Call) to the parent for collection naming
      const normalizedEntityNameForWrite = entityName.includes(":")
        ? entityName.split(":")[0]
        : entityName;
      const collectionName = `${dataSource.name}_${normalizedEntityNameForWrite}`;
      const stagingCollectionName = `${collectionName}_staging`;
      const useStaging = syncMode === "full";

      const collection = useStaging
        ? db.collection(stagingCollectionName)
        : db.collection(collectionName);

      if (useStaging) {
        // Drop staging collection if exists
        try {
          await db.collection(stagingCollectionName).drop();
        } catch {
          // Ignore if doesn't exist
        }
        await db.createCollection(stagingCollectionName);

        // Create indexes on staging collection
        const stagingCollection = db.collection(stagingCollectionName);
        await ensureCollectionIndexes(stagingCollection, logger);
      } else {
        // Ensure indexes exist for incremental sync
        await ensureCollectionIndexes(collection, logger);
      }

      let recordCount = 0;
      let lastSyncDate: Date | undefined;

      // Get last sync date for incremental
      if (syncMode === "incremental") {
        logger?.log(
          "debug",
          `Looking for last sync date in collection: ${collectionName}`,
        );
        logger?.log("debug", `Using dataSourceId filter: ${dataSource.id}`);

        const lastRecord = await db
          .collection(collectionName)
          .find({ _dataSourceId: dataSource.id })
          .sort({ _syncedAt: -1 })
          .limit(1)
          .toArray();

        logger?.log(
          "debug",
          `Found ${lastRecord.length} records with _dataSourceId: ${dataSource.id}`,
        );

        if (lastRecord.length > 0) {
          lastSyncDate = lastRecord[0]._syncedAt;
          logger?.log(
            "debug",
            `Last record _syncedAt: ${lastRecord[0]._syncedAt}`,
          );
          logger?.log(
            "debug",
            `Last record _dataSourceId: ${lastRecord[0]._dataSourceId}`,
          );
          logger?.log(
            "info",
            `Syncing ${entityName} updated after: ${lastSyncDate!.toISOString()}`,
          );
        } else {
          logger?.log(
            "warn",
            `No previous sync records found for ${entityName} with dataSourceId: ${dataSource.id}`,
          );
        }
      }

      // Create progress reporter for this entity
      const progressReporter = new ProgressReporter(
        entityName,
        undefined,
        logger,
      );

      // Fetch data from connector with retry logic
      const maxRetries = dataSource.settings?.max_retries || 3;
      const rateLimitDelay = dataSource.settings?.rate_limit_delay_ms || 200;
      await executeWithRetry(
        () =>
          connector.fetchEntity({
            entity: entityName,
            ...(lastSyncDate && { since: lastSyncDate }),
            onBatch: async batch => {
              if (batch.length === 0) return;

              // Add metadata to records
              const processedRecords = batch.map(record => ({
                ...record,
                _dataSourceId: dataSource.id,
                _dataSourceName: dataSource.name,
                _syncedAt: new Date(),
              }));

              // Prepare bulk operations
              const bulkOps = processedRecords.map(record => ({
                replaceOne: {
                  filter: {
                    id: record.id,
                    _dataSourceId: dataSource.id,
                  },
                  replacement: record,
                  upsert: true,
                },
              }));

              // Write to database
              await collection.bulkWrite(bulkOps, { ordered: false });
              recordCount += batch.length;
            },
            onProgress: (current, total) => {
              progressReporter.reportProgress(current, total);
            },
          }),
        maxRetries,
        logger,
        step, // Pass step parameter for Inngest sleep when available
        `fetch-entity-${entityName}`,
        rateLimitDelay,
      );

      // Complete the progress reporting
      progressReporter.reportComplete();

      // Hot swap for full sync
      if (useStaging) {
        // Drop the existing collection and rename staging to main
        try {
          await db.collection(collectionName).drop();
        } catch {
          // Ignore if doesn't exist
        }

        // Rename staging to main
        await db.collection(stagingCollectionName).rename(collectionName);
      }

      logger?.log(
        "info",
        `âœ… ${entityName} sync completed (${recordCount} records)`,
      );
    }

    const duration = ((Date.now() - startTime) / 1000).toFixed(2);
    logger?.log("info", `Sync completed successfully in ${duration}s`);
  } catch (error) {
    const errorMsg = `Sync failed: ${error instanceof Error ? error.message : String(error)}`;
    logger?.log("error", errorMsg, {
      stack: error instanceof Error ? error.stack : undefined,
    });
    throw new Error(errorMsg, { cause: error });
  }
  // Note: We don't close the connection here anymore - it stays in the unified pool
}
